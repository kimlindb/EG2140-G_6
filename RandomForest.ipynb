{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60657d5b",
   "metadata": {},
   "source": [
    "## Random Forest Regession Implementation step by step\n",
    "\n",
    "Most of the code is copied and modifed from the different implementations in Notes.ipynb, take aways from the previous version that needs chaning implementation to t+24h forcast instead of t+6h. This along with energy data is seen as \"true\" meaning it cannot be used for training and testing straight away. Instead you would need to have it as a seperate target which sounds like a headace because boa is affected by the market and I'm not even going to attempt. Regardless something simple coud be tested to be implemented but only if time is avalible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573b359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and preprocces data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "model_table = pd.read_csv(\"data/forecast_data_merged_2.csv\")\n",
    "# convert to datetime\n",
    "model_table[\"ref_datetime\"] = pd.to_datetime(model_table[\"ref_datetime\"])\n",
    "model_table[\"valid_datetime\"] = pd.to_datetime(model_table[\"valid_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b920f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_datetime</th>\n",
       "      <th>valid_datetime</th>\n",
       "      <th>dwd_RelativeHumidity</th>\n",
       "      <th>dwd_WindDirection_100</th>\n",
       "      <th>dwd_WindSpeed_100</th>\n",
       "      <th>dwd_CloudCover</th>\n",
       "      <th>dwd_SolarDownwardRadiation</th>\n",
       "      <th>dwd_Temperature</th>\n",
       "      <th>ncep_RelativeHumidity</th>\n",
       "      <th>ncep_WindDirection_100</th>\n",
       "      <th>ncep_WindSpeed_100</th>\n",
       "      <th>ncep_CloudCover</th>\n",
       "      <th>ncep_SolarDownwardRadiation</th>\n",
       "      <th>ncep_Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-20 00:00:00+00:00</td>\n",
       "      <td>2020-09-20 00:00:00+00:00</td>\n",
       "      <td>85.213750</td>\n",
       "      <td>62.085170</td>\n",
       "      <td>11.802604</td>\n",
       "      <td>0.450405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.646173</td>\n",
       "      <td>84.066666</td>\n",
       "      <td>58.721077</td>\n",
       "      <td>11.338991</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.940016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-20 00:00:00+00:00</td>\n",
       "      <td>2020-09-20 00:30:00+00:00</td>\n",
       "      <td>85.012270</td>\n",
       "      <td>61.726974</td>\n",
       "      <td>11.648818</td>\n",
       "      <td>0.472211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.658508</td>\n",
       "      <td>84.433334</td>\n",
       "      <td>58.494644</td>\n",
       "      <td>11.516161</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.896579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-20 00:00:00+00:00</td>\n",
       "      <td>2020-09-20 01:00:00+00:00</td>\n",
       "      <td>84.810780</td>\n",
       "      <td>61.368782</td>\n",
       "      <td>11.495032</td>\n",
       "      <td>0.494018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.670843</td>\n",
       "      <td>84.800000</td>\n",
       "      <td>58.268215</td>\n",
       "      <td>11.693331</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.853142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-20 00:00:00+00:00</td>\n",
       "      <td>2020-09-20 01:30:00+00:00</td>\n",
       "      <td>84.357895</td>\n",
       "      <td>61.111046</td>\n",
       "      <td>11.354128</td>\n",
       "      <td>0.520214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.732101</td>\n",
       "      <td>84.933334</td>\n",
       "      <td>59.550636</td>\n",
       "      <td>11.716686</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.847113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-20 00:00:00+00:00</td>\n",
       "      <td>2020-09-20 02:00:00+00:00</td>\n",
       "      <td>83.905000</td>\n",
       "      <td>60.853313</td>\n",
       "      <td>11.213223</td>\n",
       "      <td>0.546410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.793360</td>\n",
       "      <td>85.066666</td>\n",
       "      <td>60.833060</td>\n",
       "      <td>11.740043</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.841084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452755</th>\n",
       "      <td>2023-10-27 00:00:00+00:00</td>\n",
       "      <td>2023-10-27 21:30:00+00:00</td>\n",
       "      <td>80.259060</td>\n",
       "      <td>177.495090</td>\n",
       "      <td>2.620757</td>\n",
       "      <td>0.549926</td>\n",
       "      <td>-0.003020</td>\n",
       "      <td>8.741467</td>\n",
       "      <td>81.200000</td>\n",
       "      <td>203.196010</td>\n",
       "      <td>5.695431</td>\n",
       "      <td>0.144725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.939343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452756</th>\n",
       "      <td>2023-10-27 00:00:00+00:00</td>\n",
       "      <td>2023-10-27 22:00:00+00:00</td>\n",
       "      <td>79.186960</td>\n",
       "      <td>172.861190</td>\n",
       "      <td>2.756149</td>\n",
       "      <td>0.585028</td>\n",
       "      <td>-0.011098</td>\n",
       "      <td>8.610748</td>\n",
       "      <td>80.122220</td>\n",
       "      <td>209.989490</td>\n",
       "      <td>5.170937</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.804339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452757</th>\n",
       "      <td>2023-10-27 00:00:00+00:00</td>\n",
       "      <td>2023-10-27 22:30:00+00:00</td>\n",
       "      <td>79.410890</td>\n",
       "      <td>173.374680</td>\n",
       "      <td>3.079223</td>\n",
       "      <td>0.676564</td>\n",
       "      <td>-0.004312</td>\n",
       "      <td>8.533447</td>\n",
       "      <td>79.872220</td>\n",
       "      <td>208.122830</td>\n",
       "      <td>5.281261</td>\n",
       "      <td>0.369525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.769360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452758</th>\n",
       "      <td>2023-10-27 00:00:00+00:00</td>\n",
       "      <td>2023-10-27 23:00:00+00:00</td>\n",
       "      <td>79.634820</td>\n",
       "      <td>173.888170</td>\n",
       "      <td>3.402297</td>\n",
       "      <td>0.768100</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>8.456146</td>\n",
       "      <td>79.622220</td>\n",
       "      <td>206.256160</td>\n",
       "      <td>5.391584</td>\n",
       "      <td>0.523950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.734381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452759</th>\n",
       "      <td>2023-10-27 00:00:00+00:00</td>\n",
       "      <td>2023-10-27 23:30:00+00:00</td>\n",
       "      <td>81.139660</td>\n",
       "      <td>174.702400</td>\n",
       "      <td>3.583200</td>\n",
       "      <td>0.810268</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>8.438153</td>\n",
       "      <td>79.888885</td>\n",
       "      <td>202.174240</td>\n",
       "      <td>5.464283</td>\n",
       "      <td>0.662150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.823177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452760 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref_datetime            valid_datetime  \\\n",
       "0      2020-09-20 00:00:00+00:00 2020-09-20 00:00:00+00:00   \n",
       "1      2020-09-20 00:00:00+00:00 2020-09-20 00:30:00+00:00   \n",
       "2      2020-09-20 00:00:00+00:00 2020-09-20 01:00:00+00:00   \n",
       "3      2020-09-20 00:00:00+00:00 2020-09-20 01:30:00+00:00   \n",
       "4      2020-09-20 00:00:00+00:00 2020-09-20 02:00:00+00:00   \n",
       "...                          ...                       ...   \n",
       "452755 2023-10-27 00:00:00+00:00 2023-10-27 21:30:00+00:00   \n",
       "452756 2023-10-27 00:00:00+00:00 2023-10-27 22:00:00+00:00   \n",
       "452757 2023-10-27 00:00:00+00:00 2023-10-27 22:30:00+00:00   \n",
       "452758 2023-10-27 00:00:00+00:00 2023-10-27 23:00:00+00:00   \n",
       "452759 2023-10-27 00:00:00+00:00 2023-10-27 23:30:00+00:00   \n",
       "\n",
       "        dwd_RelativeHumidity  dwd_WindDirection_100  dwd_WindSpeed_100  \\\n",
       "0                  85.213750              62.085170          11.802604   \n",
       "1                  85.012270              61.726974          11.648818   \n",
       "2                  84.810780              61.368782          11.495032   \n",
       "3                  84.357895              61.111046          11.354128   \n",
       "4                  83.905000              60.853313          11.213223   \n",
       "...                      ...                    ...                ...   \n",
       "452755             80.259060             177.495090           2.620757   \n",
       "452756             79.186960             172.861190           2.756149   \n",
       "452757             79.410890             173.374680           3.079223   \n",
       "452758             79.634820             173.888170           3.402297   \n",
       "452759             81.139660             174.702400           3.583200   \n",
       "\n",
       "        dwd_CloudCover  dwd_SolarDownwardRadiation  dwd_Temperature  \\\n",
       "0             0.450405                    0.000000        13.646173   \n",
       "1             0.472211                    0.000000        13.658508   \n",
       "2             0.494018                    0.000000        13.670843   \n",
       "3             0.520214                    0.000000        13.732101   \n",
       "4             0.546410                    0.000000        13.793360   \n",
       "...                ...                         ...              ...   \n",
       "452755        0.549926                   -0.003020         8.741467   \n",
       "452756        0.585028                   -0.011098         8.610748   \n",
       "452757        0.676564                   -0.004312         8.533447   \n",
       "452758        0.768100                    0.002474         8.456146   \n",
       "452759        0.810268                    0.000469         8.438153   \n",
       "\n",
       "        ncep_RelativeHumidity  ncep_WindDirection_100  ncep_WindSpeed_100  \\\n",
       "0                   84.066666               58.721077           11.338991   \n",
       "1                   84.433334               58.494644           11.516161   \n",
       "2                   84.800000               58.268215           11.693331   \n",
       "3                   84.933334               59.550636           11.716686   \n",
       "4                   85.066666               60.833060           11.740043   \n",
       "...                       ...                     ...                 ...   \n",
       "452755              81.200000              203.196010            5.695431   \n",
       "452756              80.122220              209.989490            5.170937   \n",
       "452757              79.872220              208.122830            5.281261   \n",
       "452758              79.622220              206.256160            5.391584   \n",
       "452759              79.888885              202.174240            5.464283   \n",
       "\n",
       "        ncep_CloudCover  ncep_SolarDownwardRadiation  ncep_Temperature  \n",
       "0              0.026000                          0.0         13.940016  \n",
       "1              0.039000                          0.0         13.896579  \n",
       "2              0.052000                          0.0         13.853142  \n",
       "3              0.038500                          0.0         13.847113  \n",
       "4              0.025000                          0.0         13.841084  \n",
       "...                 ...                          ...               ...  \n",
       "452755         0.144725                          0.0          8.939343  \n",
       "452756         0.215100                          0.0          8.804339  \n",
       "452757         0.369525                          0.0          8.769360  \n",
       "452758         0.523950                          0.0          8.734381  \n",
       "452759         0.662150                          0.0          8.823177  \n",
       "\n",
       "[452760 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop the True values\n",
    "columns_to_remove = [\"dtm\", \"Solar_MW\", \"Wind_MW\", \n",
    "                    \"wind_curtailment_MW\", \"wind_potential_MW\", \n",
    "                    \"Solar_MWh_credit\", \"Wind_MWh_credit\",\n",
    "                    \"total_generation_MWh\"]\n",
    "\n",
    "model_table = model_table.drop(columns_to_remove, axis=1)\n",
    "display(model_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300026e",
   "metadata": {},
   "source": [
    "Now the fun begins, so the thing is with these forcasts is that every 6 hours we get a 50h forcast (technically more but it's not included, feel free to do so if you want). This means that the forcasts actually overlapp eachother. We need to process this before creating traning and testing splits. Unfortunetly there are a plethera of methods that can be used to handle this simplest being just cutting of the overlapping time spand to the most resent forcast, other ways consist of weight the most resent forcast being heavier, other method could be taking means of overlapping time. One could also do seperate forcasts for each of the overlaps and take the best result. But instead of writing funny words, let's do 2 different types of additional preprocessing:\n",
    "\n",
    "1. numero uno taking the most resent forcast \n",
    "2. numero dos taking the mean over overlapping forcasts.\n",
    "\n",
    "## Additional Preprocessing of data generally\n",
    "\n",
    "vÃ¤lkommen till mitt personliga helvete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e052b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Som man kan sÃ¤ga fÃ¶rsta lÃ¶sningen Ã¤r sÃ¤llan den bÃ¤sta, detta Ã¤r en fÃ¶rnyad version av mitt tiddiagre prepross scritps \n",
    "\n",
    "## le dernier est le meilleur; den senaste Ã¤r den bÃ¤sta \n",
    "# Assuming the lastest forcast is the best forcast\n",
    "X_latest_forcast = pd.DataFrame() \n",
    "num_day = model_table[\"ref_datetime\"].dt.date.nunique()\n",
    "# print(num_day)\n",
    "# 1133 good value \n",
    "\n",
    "first_day = pd.Timestamp(\"2020-09-20 00:00:00+00:00\")\n",
    "\n",
    "# Now we mask for all day, bit slow but it works\n",
    "for d in range(num_day):\n",
    "       start = (first_day + timedelta(hours=18)) + timedelta(days=d)\n",
    "       end = start + timedelta(hours=24)\n",
    "       mask = (model_table[\"ref_datetime\"] == start) &\\\n",
    "              (model_table[\"valid_datetime\"] >= start + timedelta(hours=6)) &\\\n",
    "              (model_table[\"valid_datetime\"] < end + + timedelta(hours=6))\n",
    "       X_latest_forcast = pd.concat([X_latest_forcast, model_table.loc[mask]])\n",
    "\n",
    "\n",
    "## Mean of several overlapping forcasts \n",
    "X_raw = model_table.copy()\n",
    "\n",
    "# filter for D-1\n",
    "X_raw[\"target_day\"] = X_raw[\"valid_datetime\"].dt.date\n",
    "X_raw[\"reference_day\"] = X_raw[\"ref_datetime\"].dt.date\n",
    "X_raw_filtered = X_raw[X_raw[\"reference_day\"] == (X_raw[\"target_day\"] - pd.Timedelta(days=1))]\n",
    "\n",
    "# aggregate mean, very fast very nice yes yes \n",
    "X_overlapp_mean = X_raw_filtered.groupby(\"valid_datetime\", as_index=False).agg({\n",
    "    \"dwd_RelativeHumidity\": \"mean\",\n",
    "    \"dwd_WindDirection_100\": \"mean\",\n",
    "    \"dwd_WindSpeed_100\": \"mean\",\t\n",
    "    \"dwd_CloudCover\": \"mean\",\n",
    "    \"dwd_SolarDownwardRadiation\": \"mean\",\t\n",
    "    \"dwd_Temperature\": \"mean\",\n",
    "    \"ncep_RelativeHumidity\": \"mean\",\n",
    "    \"ncep_WindDirection_100\": \"mean\",\n",
    "    \"ncep_WindSpeed_100\": \"mean\",\n",
    "    \"ncep_CloudCover\": \"mean\",\t\n",
    "    \"ncep_SolarDownwardRadiation\": \"mean\",\t\n",
    "    \"ncep_Temperature\": \"mean\",\n",
    "    \"ref_datetime\": [\"first\", \"count\"]  # Keep one value + count\n",
    "})\n",
    "\n",
    "# flatten the multiIndex columns\n",
    "X_overlapp_mean.columns = [\n",
    "    \"_\".join(col) if isinstance(col, tuple) else col \n",
    "    for col in X_overlapp_mean.columns\n",
    "]\n",
    "\n",
    "# rename \n",
    "X_overlapp_mean = X_overlapp_mean.rename(columns={\n",
    "    \"ref_datetime_first\": \"ref_datetime\",\n",
    "    \"ref_datetime_count\": \"n_forecasts\",\n",
    "    \"dwd_RelativeHumidity_mean\": \"dwd_RelativeHumidity\",\n",
    "    \"dwd_WindDirection_100_mean\": \"dwd_WindDirection_100\",\n",
    "    \"dwd_WindSpeed_100_mean\": \"dwd_WindSpeed_100\",\n",
    "    \"dwd_CloudCover_mean\": \"dwd_CloudCover\",\n",
    "    \"dwd_SolarDownwardRadiation_mean\": \"dwd_SolarDownwardRadiation\",\t\n",
    "    \"dwd_Temperature_mean\": \"dwd_Temperature\",\n",
    "    \"ncep_RelativeHumidity_mean\": \"ncep_RelativeHumidity\",\n",
    "    \"ncep_WindDirection_100_mean\": \"ncep_WindDirection_100\",\n",
    "    \"ncep_WindSpeed_100_mean\": \"ncep_WindSpeed_100\",\n",
    "    \"ncep_CloudCover_mean\": \"ncep_CloudCover\",\n",
    "    \"ncep_SolarDownwardRadiation_mean\": \"ncep_SolarDownwardRadiation\",\n",
    "    \"ncep_Temperature_mean\": \"ncep_Temperature\",\n",
    "    \"valid_datetime_\": \"valid_datetime\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f8dd9",
   "metadata": {},
   "source": [
    "Let check so see that I didn't goof "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ad45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(X_latest_forcast) # 54336 rows Ã— 14 columns, dates look good lets move on \n",
    "#display(X_overlapp_mean) # 54336 rows Ã— 14 columns, 4 overlapps as it should be for 50 hours \n",
    "\n",
    "# Gifts\n",
    "#X_overlapp_mean.to_csv(\"data/forecast_data_om.csv\", index=False)\n",
    "#X_latest_forcast.to_csv(\"data/forecast_data_lf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005150b",
   "metadata": {},
   "source": [
    "Summan av kardemumman, ovandligen av datan har blandat om ordningen pÃ¥ mÃ¥len sÃ¥ man mÃ¥ste ladda om,\n",
    "notera ocksÃ¥ att frÃ¥n modeling_table att CVS fÃ¶rkastas i bÃ¶rjan av filen nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4362ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDO WHAT WAS DONE IN prepross.py \n",
    "energy_data = pd.read_csv(\"data/Energy_Data_20200920_20231027.csv\")\n",
    "\n",
    "energy_data[\"dtm\"] = pd.to_datetime(energy_data[\"dtm\"])\n",
    "\n",
    "energy_data_dtm = energy_data[\"dtm\"]\n",
    "energy_data_Solar = energy_data[\"Solar_MW\"]\n",
    "energy_data_Wind = energy_data[\"Wind_MW\"]\n",
    "energy_data_wind_curtailment = energy_data[\"boa_MWh\"]\n",
    "\n",
    "energy_data_Solar = energy_data_Solar.sort_index()\n",
    "energy_data_Solar.interpolate(method='linear', inplace=True)\n",
    "energy_data[\"Solar_MW\"] = energy_data_Solar\n",
    "\n",
    "energy_data_Wind = energy_data_Wind.sort_index()\n",
    "energy_data_Wind.interpolate(method='linear', inplace=True)\n",
    "energy_data[\"Wind_MW\"] = energy_data_Wind\n",
    "\n",
    "energy_data_wind_curtailment = energy_data_wind_curtailment.sort_index()\n",
    "energy_data_wind_curtailment.interpolate(method='linear', inplace=True)\n",
    "energy_data[\"boa_MWh\"] = energy_data_wind_curtailment\n",
    "\n",
    "energy_data[\"Wind_MWh_credit\"] = 0.5*energy_data[\"Wind_MW\"] - energy_data[\"boa_MWh\"]\n",
    "energy_data[\"Solar_MWh_credit\"] = 0.5*energy_data[\"Solar_MW\"]\n",
    "energy_data[\"total_generation_MWh\"] = energy_data[\"Wind_MWh_credit\"] + energy_data[\"Solar_MWh_credit\"]\n",
    "\n",
    "# Now we have y, note that first day has been removed as we don't have X for the first day\n",
    "y_all = energy_data[[\"dtm\", \"Wind_MWh_credit\", \"Solar_MWh_credit\", \"total_generation_MWh\", \"boa_MWh\"]][48:]\n",
    "\n",
    "# display(y_all) # 54336 rows Ã— 5 columns, good good same num of rows,as X, dates look good, we gucci "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114839c",
   "metadata": {},
   "source": [
    "okei, hyvÃ¤ jotto\n",
    "\n",
    "Currently we've got X and y but they're pretty raw, lets start cooking. These are the steps before we're done. Some of these steps will use a pipeline.\n",
    "\n",
    "1. Verify inputs, might be redundant but in the case that X and y changes in future it can be fallaged for\n",
    "2. Split X and y into solar wind and train and test\n",
    "3. (pipe) Feauture engineering, this will have several internal steps\n",
    "4. (pipe) Quantiles\n",
    "5. (pipe) Train models\n",
    "6. (pipe) Predic\n",
    "7. Take combined forcast and display results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf223da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Verify inputs, I'm continously checking that things don't goof \n",
    "\n",
    "# 2. Split intor trainin and testing\n",
    "split_date = pd.Timestamp(\"2022-10-01 00:00:00+00:00\")\n",
    "\n",
    "# mo = mean overlapp\n",
    "X_train_mo = X_overlapp_mean[X_overlapp_mean[\"valid_datetime\"] < split_date].copy()\n",
    "X_test_mo = X_overlapp_mean[X_overlapp_mean[\"valid_datetime\"] >= split_date].copy()\n",
    "\n",
    "# lf = latest forcast\n",
    "X_train_lf = X_latest_forcast[X_latest_forcast[\"valid_datetime\"] < split_date].copy()\n",
    "X_test_lf = X_latest_forcast[X_latest_forcast[\"valid_datetime\"] >= split_date].copy()\n",
    "\n",
    "wind_features = [\"valid_datetime\",\"dwd_RelativeHumidity\", \"dwd_WindDirection_100\", \"dwd_WindSpeed_100\",\n",
    "                \"ncep_RelativeHumidity\", \"ncep_WindDirection_100\", \"ncep_WindSpeed_100\"]\n",
    "\n",
    "solar_features = [\"valid_datetime\",\"dwd_CloudCover\", \"dwd_SolarDownwardRadiation\", \"dwd_Temperature\",\n",
    "                \"ncep_CloudCover\", \"ncep_SolarDownwardRadiation\", \"ncep_Temperature\"]\n",
    "\n",
    "# seperate into solar and wind\n",
    "X_train_solar_mo = X_train_mo[solar_features]\n",
    "X_test_solar_mo = X_test_mo[solar_features]\n",
    "X_train_wind_mo = X_train_mo[wind_features]\n",
    "X_test_solar_mo = X_test_mo[wind_features]\n",
    "\n",
    "X_train_solar_lf = X_train_lf[solar_features]\n",
    "X_test_solar_lf = X_test_lf[solar_features]\n",
    "X_train_wind_lf = X_train_lf[wind_features]\n",
    "X_test_wind_lf = X_test_lf[wind_features]\n",
    "\n",
    "# take out targets\n",
    "y_all_train = y_all[y_all[\"dtm\"] < split_date].copy()\n",
    "y_all_test = y_all[y_all[\"dtm\"] >= split_date].copy()\n",
    "\n",
    "y_train_solar = y_all_train[\"Solar_MWh_credit\"]\n",
    "y_test_solar = y_all_test[\"Solar_MWh_credit\"]\n",
    "\n",
    "y_train_wind = y_all_train[\"Wind_MWh_credit\"]\n",
    "y_test_wind = y_all_test[\"Wind_MWh_credit\"]\n",
    "\n",
    "# wps = wind plus solar \n",
    "y_train_wps = y_all_train[\"total_generation_MWh\"]\n",
    "y_test_wps = y_all_test[\"total_generation_MWh\"]\n",
    "\n",
    "# time vectors used for verification of dates\n",
    "t_train = y_all_train[\"dtm\"]\n",
    "t_test = y_all_test[\"dtm\"]\n",
    "\n",
    "# Check for goofy \n",
    "#display(t_train)\n",
    "#display(X_train_solar_lf)\n",
    "#print(len(t_train))\n",
    "#print(len(X_train_solar_lf))\n",
    "# Same lenght and valid datetime lines up with dtm\n",
    "\n",
    "#display(t_test)\n",
    "#display(X_test_solar_lf)\n",
    "#print(len(t_test))\n",
    "#print(len(X_test_solar_lf))\n",
    "# Same lenght and valid datetime lines up with dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d51a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom classes\n",
    "\n",
    "# 3. Feauter Engineering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 3.1 Costum cyclic transformer, transforms wind if necessary\n",
    "class CyclicFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, datetime_col = \"valid_datetime\", wind_dir_cols=None, drop_original=True):\n",
    "        self.datetime_col = datetime_col\n",
    "        self.wind_dir_cols = wind_dir_cols or []\n",
    "        self.drop_original = drop_original\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        dt_col = pd.to_datetime(X_[self.datetime_col])\n",
    "        X_[\"hour_sin\"] = np.sin(2*np.pi*(dt_col.dt.hour + dt_col.dt.minute/60)/24)\n",
    "        X_[\"hour_cos\"] = np.cos(2*np.pi*(dt_col.dt.hour + dt_col.dt.minute/60)/24)\n",
    "        X_[\"weekday_sin\"] = np.sin(2*np.pi * dt_col.dt.weekday/7)\n",
    "        X_[\"weekday_cos\"] = np.cos(2*np.pi*dt_col.dt.weekday/7)\n",
    "        X_[\"month_sin\"] = np.sin(2*np.pi*dt_col.dt.month/12)\n",
    "        X_[\"month_cos\"] = np.cos(2*np.pi*dt_col.dt.month/12)\n",
    "        \n",
    "        # Add cyclic wind direction features (if any)\n",
    "        for col in self.wind_dir_cols:\n",
    "            radians = np.deg2rad(X_[col])\n",
    "            X_[f\"{col}_sin\"] = np.sin(radians)\n",
    "            X_[f\"{col}_cos\"] = np.cos(radians)\n",
    "            dt_col = pd.to_datetime(X_[self.datetime_col])\n",
    "\n",
    "        # Drop original datetime and wind_dir columns if specified\n",
    "        if self.drop_original:\n",
    "            drop_cols = [self.datetime_col] + self.wind_dir_cols\n",
    "            X_ = X_.drop(columns=drop_cols)\n",
    "\n",
    "        return X_\n",
    "\n",
    "\n",
    "# 3.2 Lag desired columns\n",
    "class LagFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, lags=[-2, -1, 0, 1, 2], drop_original=True):\n",
    "        self.columns = columns\n",
    "        self.lags = lags\n",
    "        self.drop_original = drop_original\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        for col in self.columns:\n",
    "            for lag in self.lags:\n",
    "                X_[f'{col}_lag{lag}'] = X_[col].shift(lag)\n",
    "\n",
    "        if self.drop_original:\n",
    "            X_.drop(columns=self.columns, inplace=True)\n",
    "        \n",
    "        return X_\n",
    "\n",
    "\n",
    "# 3.3 Safe Scaler and Imputer that doesn't remove column names \n",
    "class SafeSI(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.feature_names_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X)\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        self.scaler.fit(X_imputed)\n",
    "        if hasattr(X, 'columns'):\n",
    "            self.feature_names_ = X.columns.tolist()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        X_scaled = self.scaler.transform(X_imputed)\n",
    "        if self.feature_names_:\n",
    "            return pd.DataFrame(X_scaled, \n",
    "                             columns=self.feature_names_,\n",
    "                             index=X.index if hasattr(X, 'index') else None)\n",
    "        return X_scaled\n",
    "\n",
    "# 4. Quantiles, there's no prepackaged Random Forest Quantile Regression\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class RandomForestQuantileRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, \n",
    "                 quantiles=[0.5], \n",
    "                 n_estimators=100,\n",
    "                 max_depth=14,\n",
    "                 min_samples_split=10,\n",
    "                 random_state=34,\n",
    "                 criterion='squared_error',\n",
    "                 n_jobs=-1,\n",
    "                 **rf_params):\n",
    "\n",
    "        self.quantiles = np.sort(np.atleast_1d(quantiles))\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.n_jobs = n_jobs\n",
    "        self.rf_params = rf_params\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.model_ = RandomForestRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            random_state=self.random_state,\n",
    "            criterion=self.criterion,\n",
    "            n_jobs=self.n_jobs,\n",
    "            **self.rf_params\n",
    "        ).fit(X, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # Memory-efficient prediction collection\n",
    "        preds = np.zeros((X.shape[0], self.model_.n_estimators))\n",
    "        for i, tree in enumerate(self.model_.estimators_):\n",
    "            preds[:, i] = tree.predict(X)\n",
    "            \n",
    "        return np.percentile(preds, [q*100 for q in self.quantiles], axis=1).T\n",
    "    \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.model_.feature_importances_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b072640",
   "metadata": {},
   "source": [
    "brrrrrrrrrrrrrr\n",
    "\n",
    "Additional preprocessing steps:\n",
    "1. Cyclic (works)\n",
    "2. lag (works)\n",
    "3. Imputer (works, but returns an array)\n",
    "4. Scaler (works, but returns an array)\n",
    "\n",
    "Great well I don't want arrays, so I'll create another costum function, 3 and 4 are now a class\n",
    "\n",
    "Now we can start running the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e403cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "wind_dir_col = [\"dwd_WindDirection_100\", \"ncep_WindDirection_100\"]\n",
    "wind_lag_col = [\"dwd_RelativeHumidity\", \"dwd_WindSpeed_100\",\n",
    "                \"ncep_RelativeHumidity\", \"ncep_WindSpeed_100\"]\n",
    "solar_lag_col = [\"dwd_CloudCover\", \"dwd_SolarDownwardRadiation\", \"dwd_Temperature\",\n",
    "                 \"ncep_CloudCover\", \"ncep_SolarDownwardRadiation\", \"ncep_Temperature\"]\n",
    "\n",
    "# Pipeline for wind data\n",
    "feature_engineering_wind = Pipeline([\n",
    "    (\"cyclic\", CyclicFeatureAdder(datetime_col=\"valid_datetime\", wind_dir_cols=wind_dir_col, drop_original=True)),\n",
    "    (\"lag\", LagFeatureAdder(columns=wind_lag_col, drop_original=True)),\n",
    "    (\"imputer and scaler\", SafeSI())\n",
    "])\n",
    "\n",
    "# Pipeline for solar data\n",
    "feature_engineering_solar = Pipeline([\n",
    "    (\"cyclic\", CyclicFeatureAdder(datetime_col=\"valid_datetime\", wind_dir_cols=None, drop_original=True)),\n",
    "    (\"lag\", LagFeatureAdder(columns=solar_lag_col, drop_original=True)),\n",
    "    (\"imputer and scaler\", SafeSI())\n",
    "])\n",
    "\n",
    "# Look at transformed sets they, NaNs are pretty \n",
    "#X_train_transformed = feature_engineering_solar.fit_transform(X_train_solar_mo)\n",
    "#display(X_train_transformed)\n",
    "#X_train_transformed = feature_engineering_wind.fit_transform(X_train_wind_mo)\n",
    "#display(X_train_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fee75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸš€ SOLAR FORECAST PROCESSING\n",
      "ðŸ“Š Training samples: 35,520 | Test samples: 18,816\n",
      "\n",
      "ðŸ”§ [1/3] Feature Engineering\n",
      "Transformer steps: ['cyclic', 'lag', 'imputer and scaler']\n",
      "âœ… Features generated: 36\n",
      "ðŸ•’ Time elapsed: 0.1s\n",
      "\n",
      "ðŸŽ¯ [2/3] Model Training\n",
      "Model: RandomForestQuantileRegressor\n",
      "Quantiles: [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "def run_forecast(X_train, y_train, X_test, feature_pipe, model, name, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\" {name.upper()} FORECAST PROCESSING\")\n",
    "        print(f\" Training samples: {len(X_train):,} | Test samples: {len(X_test):,}\")\n",
    "        start_time = time.time()\n",
    "    \n",
    "    # Feature engineering\n",
    "    if verbose:\n",
    "        print(\"\\n [1/3] Feature Engineering\")\n",
    "        print(f\"Transformer steps: {[name for name, _ in feature_pipe.steps]}\")\n",
    "    \n",
    "    X_train_processed = feature_pipe.fit_transform(X_train, y_train)\n",
    "    X_test_processed = feature_pipe.transform(X_test)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\" Features generated: {X_train_processed.shape[1]}\")\n",
    "        print(f\" Time elapsed: {time.time()-start_time:.1f}s\")\n",
    "    \n",
    "    # Model training\n",
    "    if verbose:\n",
    "        print(\"\\n [2/3] Model Training\")\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        print(f\"Quantiles: {model.quantiles}\")\n",
    "    \n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\" Training completed\")\n",
    "        print(f\" Time elapsed: {time.time()-start_time:.1f}s\")\n",
    "    \n",
    "    # Prediction\n",
    "    if verbose:\n",
    "        print(\"\\n [3/3] Generating Predictions\")\n",
    "    \n",
    "    predictions = model.predict(X_test_processed)\n",
    "    result_df = pd.DataFrame(\n",
    "        predictions,\n",
    "        columns=[f'q_{int(q*100)}' for q in model.quantiles],\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Predictions generated\")\n",
    "        print(f\"Total processing time: {time.time()-start_time:.1f}s\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Configure verbose output\n",
    "VERBOSE = True\n",
    "\n",
    "# 1. Solar Forecast\n",
    "predictions_solar = run_forecast(\n",
    "    X_train_solar_lf, y_train_solar, X_test_solar_lf,\n",
    "    feature_engineering_solar, quantile_regressor, \"Solar\", VERBOSE\n",
    ")\n",
    "\n",
    "# 2. Wind Forecast\n",
    "predictions_wind = run_forecast(\n",
    "    X_train_wind_lf, y_train_wind, X_test_wind_lf,\n",
    "    feature_engineering_solar, quantile_regressor, \"Wind\", VERBOSE\n",
    ")\n",
    "\n",
    "# 3. Combine Results\n",
    "if VERBOSE:\n",
    "    print(\"\\n COMBINING SOLAR + WIND PREDICTIONS\")\n",
    "    combine_start = time.time()\n",
    "\n",
    "total_predictions = predictions_solar.add(predictions_wind)\n",
    "total_predictions['true_generation'] = y_test_wps\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"Combined predictions | Shape: {total_predictions.shape}\")\n",
    "    print(f\"Time elapsed: {time.time()-combine_start:.1f}s\")\n",
    "\n",
    "# 4. Pinball Loss Calculation\n",
    "if VERBOSE:\n",
    "    print(\"\\n CALCULATING PINBALL LOSS\")\n",
    "    pinball_start = time.time()\n",
    "\n",
    "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "pinball_scores = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    q_label = f'q_{int(q*100)}'\n",
    "    loss = pinball_loss(\n",
    "        total_predictions['true_generation'],\n",
    "        total_predictions[q_label],\n",
    "        q\n",
    "    )\n",
    "    pinball_scores[q_label] = loss\n",
    "    if VERBOSE:\n",
    "        print(f\"{q_label}: {loss:.4f}\")\n",
    "\n",
    "total_pinball = np.mean(list(pinball_scores.values()))\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"\\n Total Pinball Score: {total_pinball:.4f}\")\n",
    "    print(f\"Calculation time: {time.time()-pinball_start:.1f}s\")\n",
    "\n",
    "# 5. Visualization\n",
    "if VERBOSE:\n",
    "    print(\"\\n GENERATING VISUALIZATION\")\n",
    "    plot_start = time.time()\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "N = 150\n",
    "index_slice = total_predictions.index[-N:]\n",
    "true_values = total_predictions['true_generation'][-N:]\n",
    "median_pred = total_predictions['q_50'][-N:]\n",
    "q10_pred = total_predictions['q_10'][-N:]\n",
    "q90_pred = total_predictions['q_90'][-N:]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(index_slice, true_values, label=\"True Generation\", color=\"black\", linewidth=2)\n",
    "plt.plot(index_slice, median_pred, label=\"Predicted Median (q50)\", color=\"royalblue\", linestyle=\"--\")\n",
    "plt.fill_between(index_slice, q10_pred, q90_pred, color=\"skyblue\", alpha=0.4, label=\"80% PI (q10-q90)\")\n",
    "\n",
    "plt.title(f\"Wind + Solar Generation Forecast\\nLast {N} Points | Total Pinball Score: {total_pinball:.4f}\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Generation (MWh)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"Plot generation time: {time.time()-plot_start:.1f}s\")\n",
    "    print(\"\\n FORECAST PROCESS COMPLETED\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e5909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            0\n",
      "1            1\n",
      "2            2\n",
      "3            3\n",
      "4            4\n",
      "         ...  \n",
      "35515    35515\n",
      "35516    35516\n",
      "35517    35517\n",
      "35518    35518\n",
      "35519    35519\n",
      "Name: valid_datetime, Length: 35520, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test =X_train_solar_mo[\"valid_datetime\"]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7f4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
